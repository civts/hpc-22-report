\section{Benchmarking}
\label{sec:benchmarking}

The benchmarks were run on the University of Trento's High Performance Computing cluster
%\footnote{\url{https://unitrento.service-now.com/unitrento?id=unitrento_service_offering_dettaglio&category_id=68561769c33785109a8d151ce0013147&sys_id=14bb84f0c398d5104cbb7055df013133}}
.

The following combinations of parameters were considered:

\begin{table}[ht]
  \centering
  \caption{Simulation parameters}
  \begin{tabular}{l|l}
    \hline
    Parameter                                        & Range                                    \\
    \hline
    CPU(s)                                           & {1, 2, 4, 8, 16}                         \\
    Chunk(s)                                         & {1, 2, 4, 8, 16}                         \\
    Placing strategy \cite[ยง4.7.1]{pbs_super_manual} & {pack, scatter, pack:excl, scatter:excl} \\
    Input size (number of points)                    & {1M, 5M, 10M, 15M }                      \\
    \hline
  \end{tabular}
\end{table}

The jobs were always submitted asking for 1GB of RAM and with a maximum execution time set to two minutes by default, but increased to five if we were running more than 64 processes.


\subsection{Execution Time}

% We saw the execution time varying like abcd
% \begin{table}[ht]
%   \centering
%   \caption{Aggregate simulation results}
%   \begin{tabular}{l|l|l|l}
%     % or this to span the entire width \begin{tabularx}{\columnwidth}{X|X|X|X}
%     \hline
%     Chunks & Cores & Input size & Runtime (seconds) \\
%     \hline
%     2      & 2     & 1M         & 10                \\
%     \hline
%   \end{tabular}
% \end{table}

In Fig. \ref{fig:exec_time} we reported the total execution time in function of the number of processes for the four placing strategy.
It can be seen that the \textit{pack} strategy gives better performances than the \textit{scatter} strategy, however
the latter shows a higher improvement in runtime as the number of processes increase. Regarding the \textit{excl} strategies
\textit{pack} and \textit{scatter} give very similar results and there is no clear difference in the runtime.
It is interesting to look at the runtime when using the $1M$ dataset. In particular, when we use the \textit{scatter} strategy we can
see that the runtime increases when we have more than $8$ processes since the parallel overhead becomes more important and the dataset is too
small to have an increase in performances when increasing the number of processes.

\subsection{Speedup}
The speedup is an important measure in the evaluation of a parallel algorithm.
It is defined as the ratio of the serial runtime for solving a problem to the time taken by the
parallel algorithm to solve the same problem on p processors:
\begin{equation}
  Speedup = \frac{T_{serial}}{T_{parallel}}
\end{equation}
A value of speedup lower than 1 means that the parallelization did not provide a gain in execution time
since the parallel algorithm takes more time to solve the problem than the serial algorithm.
Ideally we would like to have an infinite speedup, however any speedup measure greater than one would
mean that the parallelization has been successful.

By looking at Fig. \ref{fig:speedup} we can see that with the $5M$ and $15M$ datasets our implementation reaches a speedup of $~4$ for
all the strategies using $32$ processes, except for the \textit{scatter} strategy in which the speedup
is slightly below $4$.
The $10M$ dataset reaches a slightly lower speedup, while the $1M$ dataset reaches the lowest speedup, especially
with an high number of processes. This is due to the communication overhead, which becomes more and more significant as we increase the number of processes. We find supporting evidence of this in the fact that the speedup decreases more rapidly when the placing strategy is set to `scatter' instead of `pack' -meaning that we are running the code on machines that are further from each other-.
Unfortunately we could not try the \textit{pack} strategies with more than $32$ processes due to the cluster limitations.

\subsection{Efficiency}
Efficiency is defined as the ratio of speedup to the
number of processors and it measures the fraction of
time for which a processor is usefully utilized.

\begin{equation}
  \begin{split}
    Efficiency = \frac{Speedup}{N_{processors}} \\
    = \frac{T_{serial}}{T_{parallel} * N_{processors}}
  \end{split}
\end{equation}

In Fig. \ref{fig:efficiency} we reported the efficiency of our implementation.
An efficiency of one means that every processor is fully utilized and we can see that
this case happens only sometimes when one processor is used.
For all the placing strategies we can spot the same trend: the efficiency rapidly declines as we
increase the number of processes. This is because when we double the number of processes the speedup should
double too to keep the efficiency high, however this does not happen due to the parallel overhead.
As for speedup, it is interesting to see that the efficiencies obtained with the $5M$ and $15M$ datasets are very similar and are
better than the $10M$ and $1M$ efficiencies.

\subsection{Scalability}
Our implementation does not shows a good scalability, in fact by increasing the
number of processes the efficiency drops a lot and it approaches the zero with
$128$ and $256$ processes. The reason for this is that the speedup increases linearly as
the number of processes double so, since the efficiency is inversely correlated to the number
of processes, it decreases in a linear way as we increase the number of processes ??.
To have a strong scalability we would need to have a speedup that doubles every time we double the
number of processes, however this is very difficult to achieve if the problem
is not fully parallelizable.